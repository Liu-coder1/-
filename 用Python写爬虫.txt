1.有的网页有防止非浏览器访问的设定，用爬虫访问需要伪装成浏览器。
  需要自定义网页请求报头；F12查看要爬取的网页的User-Agent（浏览器名称）信息；
  
  服务器通过User-Agent就能知道请求是哪种浏览器发送的。如果是爬虫那User-Agent就是Python，对于有   反爬机制的网站来说就要将这个值设置为一些浏览器的值伪装成浏览器去访问。
  
2.url：统一资源定位符。
  scheme：//host：port/path/？query-string=xxx#anchor
  scheme：代表访问协议（http、https等）
  host：主机名，域名（www.baidu.com）
  port：端口号，浏览器默认使用80端口，在url中省去
  path：查找路径。
  query-string：查询字符串
  anchor：锚点，前端用来做页面定位。
  在浏览器中请求一个url，浏览器会对url进行编码。除英文字母和数字和部分符号外，其它字符全部用百  分号+十六进制码进行编码（无法识别中文）

3.响应状态码：
  200 请求正常，正常返回数据
  301 永久重定向（比如访问www.jingdong.com会重定向到www.jd.com）也就是自动跳转。
  302 临时重定向（比如访问需要登录的页面但是没有登录就会跳到登录界面）临时自动跳转。
  400 请求的url在服务器上找不到，请求url错误
  403 服务器拒绝访问，权限不够
  500 服务器内部错误，可能出现bug

4.chrome浏览器抓包工具（审查元素）
  Elements：最终呈现的网页数据，有时候网页数据是通过ajax请求得到，elements下的数据不能完全相信
  Console：打印网页的一些信息
  Sources：整个网页所加载的所有文件
  Network：查看整个网页发送的所有网络请求。